{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datast 5 : breastCancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score\n",
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importinng the PlumberOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from Plumber import PlumberOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data and performing feature engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (455, 30)\n",
      "Test set shape: (114, 30)\n"
     ]
    }
   ],
   "source": [
    "# Load Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# Optional: Split for holdout evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Split the data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the performance and efficiency of the PlumberOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A single model takes  0.07993388175964355 seconds to run\n",
      "Suggested number of trails is 150 \n",
      "Best Parameters: {'n_estimators': 50, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Best Score: 0.960439177413733\n",
      "A single model takes  0.06674408912658691 seconds to run\n",
      "Suggested number of trails is 150 \n",
      "Best Parameters: {'n_estimators': 50, 'max_depth': 11, 'min_samples_split': 8, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Best Score: 0.960439177413733\n",
      "A single model takes  0.06676101684570312 seconds to run\n",
      "Suggested number of trails is 150 \n",
      "Best Parameters: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Best Score: 0.9626466829324968\n",
      "A single model takes  0.06832599639892578 seconds to run\n",
      "Suggested number of trails is 150 \n",
      "Best Parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Best Score: 0.9626466829324968\n",
      "A single model takes  0.06885409355163574 seconds to run\n",
      "Suggested number of trails is 150 \n",
      "Best Parameters: {'n_estimators': 100, 'max_depth': 15, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'log2'}\n",
      "Best Score: 0.960439177413733\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "*************\n",
      "Execution Time: mean=24.88 seconds, std=8.810285113146968 seconds, max=39.8172709941864 seconds, min=13.685577392578125 seconds\n",
      "CPU Usage: mean=7.74%, std=1.11%, max=8.80%, min=5.70%\n",
      "Memory Usage: mean=1.52 MB, std=7.54 MB, max=8.48 MB, min=-13.14 MB\n",
      "Validation Score: mean=0.956140350877193 , std=0.0 , max=0.956140350877193 , min=0.956140350877193 \n"
     ]
    }
   ],
   "source": [
    "# Arrays to store execution times, memory usage, CPU usage, and evaluatin scores\n",
    "plumber_time = np.zeros(5,'float')\n",
    "plumber_mem = np.zeros(5,'float')\n",
    "plumber_cpu =  np.zeros(5,'float')\n",
    "plumber_score = np.zeros(5,'float')\n",
    "for i in range(5):\n",
    "    opt = PlumberOptimizer(X_train, y_train)\n",
    "\n",
    "    # Monitor the current process, record the start time, initial CPU usage, and memory usage in MB\n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_time = time.time()\n",
    "    start_cpu = process.cpu_percent(interval=None)\n",
    "    start_mem = process.memory_info().rss / (1024 * 1024) \n",
    "\n",
    "\n",
    "    best = opt.optimize()\n",
    "\n",
    "    end_time = time.time()\n",
    "    end_cpu = process.cpu_percent(interval=None)\n",
    "    end_mem = process.memory_info().rss / (1024 * 1024)  \n",
    "\n",
    "    # Calculating execution time, memory usage, and CPU usage\n",
    "    plumber_time[i] = end_time - start_time\n",
    "    plumber_mem[i] = end_mem - start_mem\n",
    "    plumber_cpu[i] = end_cpu - start_cpu\n",
    "\n",
    "    # Training and evaluating the model using the best parameters\n",
    "    rf = RandomForestClassifier(**best , random_state=42 )\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "\n",
    "    plumber_score[i] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# summary for the Plumber Optimizer\n",
    "print(\"\\n\" *3)\n",
    "print(\"*************\")\n",
    "print(f\"Execution Time: mean={np.mean(plumber_time):.2f} seconds, std={np.std(plumber_time)} seconds, max={np.max(plumber_time)} seconds, min={np.min(plumber_time)} seconds\")\n",
    "print(f\"CPU Usage: mean={np.mean(plumber_cpu):.2f}%, std={np.std(plumber_cpu):.2f}%, max={np.max(plumber_cpu):.2f}%, min={np.min(plumber_cpu):.2f}%\")\n",
    "print(f\"Memory Usage: mean={np.mean(plumber_mem):.2f} MB, std={np.std(plumber_mem):.2f} MB, max={np.max(plumber_mem):.2f} MB, min={np.min(plumber_mem):.2f} MB\")\n",
    "print(f\"Validation Score: mean={np.mean(plumber_score)} , std={np.std(plumber_score)} , max={np.max(plumber_score)} , min={np.min(plumber_score)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the performance and efficiency of the GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: 131.57 seconds\n",
      "CPU Usage: 3.20%\n",
      "Memory Usage: -9.27 MB\n",
      "Validation Accuracy: 0.956140350877193\n",
      "{'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    }
   ],
   "source": [
    "# Defining GridSearch parameters\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],  \n",
    "    'max_features': ['sqrt', 'log2'],  \n",
    "    'max_depth': [10, 20, 30, 40, 50],  \n",
    "    'min_samples_split': [2, 5, 10, 12, 16, 20], \n",
    "    'min_samples_leaf': [2, 5, 6, 8,12,16, 20]  \n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "# Monitor the current process, record the start time, initial CPU usage, and memory usage in MB\n",
    "process = psutil.Process(os.getpid())\n",
    "start_time = time.time()\n",
    "start_cpu = process.cpu_percent(interval=None)\n",
    "start_mem = process.memory_info().rss / (1024 * 1024)  \n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "end_cpu = process.cpu_percent(interval=None)\n",
    "end_mem = process.memory_info().rss / (1024 * 1024) \n",
    "\n",
    "# Print metrics for GridSearch\n",
    "print(f\"Execution Time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"CPU Usage: {end_cpu - start_cpu:.2f}%\")\n",
    "print(f\"Memory Usage: {end_mem - start_mem:.2f} MB\")\n",
    "\n",
    "\n",
    "# Training the model with the best parameters and evaluate\n",
    "rf = RandomForestClassifier(** grid_search.best_params_, random_state=42 )\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the performance and efficiency of the RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time: mean=14.21 seconds, std=0.45044322324710945 seconds, max=15.09384822845459 seconds, min=13.823386192321777 seconds\n",
      "CPU Usage: mean=5.06%, std=0.51%, max=5.60%, min=4.20%\n",
      "Memory Usage: mean=0.43 MB, std=0.84 MB, max=2.11 MB, min=0.00 MB\n",
      "Validation Score: mean=0.9543859649122807 , std=0.0035087719298245723 , max=0.956140350877193 , min=0.9473684210526315 \n"
     ]
    }
   ],
   "source": [
    "# Defining the parameter for RandomizedSearch\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(50, 500),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': np.arange(10, 31),\n",
    "    'min_samples_split': np.arange(2, 50),\n",
    "    'min_samples_leaf': np.arange(1, 50)\n",
    "}\n",
    "# Arrays to store execution times, memory usage, CPU usage, and evaluation scores\n",
    "rand_time = np.zeros(5,'float')\n",
    "rand_mem = np.zeros(5,'float')\n",
    "rand_cpu =  np.zeros(5,'float')\n",
    "rand_score = np.zeros(5,'float')\n",
    "for i in range(5):\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist,\n",
    "                                    n_iter=250, cv=3, n_jobs=-1)\n",
    "    \n",
    "    # Monitor the current process, record the start time, initial CPU usage, and memory usage in MB\n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_time = time.time()\n",
    "    start_cpu = process.cpu_percent(interval=None)\n",
    "    start_mem = process.memory_info().rss / (1024 * 1024)  \n",
    "\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    end_time = time.time()\n",
    "    end_cpu = process.cpu_percent(interval=None)\n",
    "    end_mem = process.memory_info().rss / (1024 * 1024)  \n",
    "\n",
    "    # Calculating execution time, memory usage, and CPU usage\n",
    "    rand_time[i] = end_time - start_time\n",
    "    rand_mem[i] = end_mem - start_mem\n",
    "    rand_cpu[i] = end_cpu - start_cpu\n",
    "\n",
    "    # Training the model with the best parameters and evaluate\n",
    "    rf = RandomForestClassifier(** random_search.best_params_ , random_state=42 )\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "\n",
    "    rand_score[i] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "#RandomizedSearchCV results\n",
    "print(f\"Execution Time: mean={np.mean(rand_time):.2f} seconds, std={np.std(rand_time)} seconds, max={np.max(rand_time)} seconds, min={np.min(rand_time)} seconds\")\n",
    "print(f\"CPU Usage: mean={np.mean(rand_cpu):.2f}%, std={np.std(rand_cpu):.2f}%, max={np.max(rand_cpu):.2f}%, min={np.min(rand_cpu):.2f}%\")\n",
    "print(f\"Memory Usage: mean={np.mean(rand_mem):.2f} MB, std={np.std(rand_mem):.2f} MB, max={np.max(rand_mem):.2f} MB, min={np.min(rand_mem):.2f} MB\")\n",
    "print(f\"Validation Score: mean={np.mean(rand_score)} , std={np.std(rand_score)} , max={np.max(rand_score)} , min={np.min(rand_score)} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the performance and efficiency of the BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "Execution Time: mean=100.02 seconds, std=4.588859426553743 seconds, max=104.7781810760498 seconds, min=93.81323409080505 seconds\n",
      "CPU Usage: mean=1010.76%, std=3.82%, max=1016.40%, min=1004.60%\n",
      "Memory Usage: mean=75.17 MB, std=137.24 MB, max=349.53 MB, min=1.17 MB\n",
      "Validation Score: mean=0.9526315789473683 , std=0.004297350425935414 , max=0.956140350877193 , min=0.9473684210526315 \n"
     ]
    }
   ],
   "source": [
    "# Define the parameter for BayesSearchCV\n",
    "param_dist = {\n",
    "    'n_estimators': Integer(50, 500),\n",
    "    'max_features': Categorical(['sqrt', 'log2']),\n",
    "    'max_depth': Integer(10, 50),\n",
    "    'min_samples_split': Integer(2, 50),\n",
    "    'min_samples_leaf': Integer(1, 50)\n",
    "}\n",
    "# Arrays to store execution times, memory usage, CPU usage, and evaluation scores\n",
    "rand_time = np.zeros(5,'float')\n",
    "rand_mem = np.zeros(5,'float')\n",
    "rand_cpu =  np.zeros(5,'float')\n",
    "rand_score = np.zeros(5,'float')\n",
    "for i in range(5):\n",
    "    print(i)\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    bayes_search = BayesSearchCV(estimator=rf, search_spaces=param_dist,\n",
    "                             n_iter=70, cv=3, n_jobs=-1)\n",
    "\n",
    "    # Monitor the current process, record the start time, initial CPU usage, and memory usage in MB\n",
    "    process = psutil.Process(os.getpid())\n",
    "    start_time = time.time()\n",
    "    start_cpu = process.cpu_percent(interval=None)\n",
    "    start_mem = process.memory_info().rss / (1024 * 1024) \n",
    "\n",
    "    bayes_search.fit(X_train, y_train)\n",
    "\n",
    "    end_time = time.time()\n",
    "    end_cpu = process.cpu_percent(interval=None)\n",
    "    end_mem = process.memory_info().rss / (1024 * 1024)  \n",
    "\n",
    "    # Calculating execution time, memory usage, and CPU usage\n",
    "    rand_time[i] = end_time - start_time\n",
    "    rand_mem[i] = end_mem - start_mem\n",
    "    rand_cpu[i] = end_cpu - start_cpu\n",
    "\n",
    "    # Training the model with the best parameters and evaluate\n",
    "    rf = RandomForestClassifier(** bayes_search.best_params_ , random_state=42 )\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "\n",
    "\n",
    "    rand_score[i] = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# BayesSearchCV results\n",
    "print(f\"Execution Time: mean={np.mean(rand_time):.2f} seconds, std={np.std(rand_time)} seconds, max={np.max(rand_time)} seconds, min={np.min(rand_time)} seconds\")\n",
    "print(f\"CPU Usage: mean={np.mean(rand_cpu):.2f}%, std={np.std(rand_cpu):.2f}%, max={np.max(rand_cpu):.2f}%, min={np.min(rand_cpu):.2f}%\")\n",
    "print(f\"Memory Usage: mean={np.mean(rand_mem):.2f} MB, std={np.std(rand_mem):.2f} MB, max={np.max(rand_mem):.2f} MB, min={np.min(rand_mem):.2f} MB\")\n",
    "print(f\"Validation Score: mean={np.mean(rand_score)} , std={np.std(rand_score)} , max={np.max(rand_score)} , min={np.min(rand_score)} \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
